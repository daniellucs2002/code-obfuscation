{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/python/final/jsonl/train/python_train_0.jsonl', 'r') as f:\n",
    "    sample_file = f.readlines()\n",
    "pprint(json.loads(sample_file[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# files = sorted(Path('data/').glob('**/*.jsonl'))\n",
    "files = sorted(Path('data/python/').glob('**/*.jsonl'))\n",
    "\n",
    "columns_long_list = ['repo', 'path', 'url', 'code', \n",
    "                     'code_tokens', 'docstring', 'docstring_tokens', \n",
    "                     'language', 'partition']\n",
    "\n",
    "def jsonl_list_to_dataframe(file_list, columns=columns_long_list):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f, \n",
    "                                   orient='records', \n",
    "                                #    compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)\n",
    "\n",
    "df = jsonl_list_to_dataframe(files, columns_long_list)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on a python file first\n",
    "print(df.language.value_counts())\n",
    "\n",
    "# What happens next?\n",
    "# 1. filter out code_tokens that start with #(comments)\n",
    "# 2. concatenate all the tokens into a code string\n",
    "# done with further processing\n",
    "\n",
    "columns_short_list = ['code_tokens']\n",
    "code = jsonl_list_to_dataframe(files, columns_short_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code['filtered_code_tokens'] = [[token for token in row if len(token) > 0 and token[0] != '#']\n",
    "                                for row in code['code_tokens']]\n",
    "code.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code['code_string'] = [' '.join(row) for row in code['filtered_code_tokens']]\n",
    "code.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce the local language model to do the \"multi mask filling\"\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "\n",
    "print(tokenizer.mask_token_id)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"if ( <mask> is not None ) <mask> ( x > 1 )\"  # simulate the multi mask scenario\n",
    "token_ids = tokenizer.encode(code, return_tensors='pt')\n",
    "masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "masked_pos = [mask.item() for mask in masked_position]\n",
    "# masked_pos  # [3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(token_ids)\n",
    "output.logits.shape  # output[0].shape is torch.Size([1, 15, 50265])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = output[0].squeeze()\n",
    "\n",
    "list_of_list = []  # multiple guessings for each masked token\n",
    "for mask_index in masked_pos:\n",
    "    mask_hidden_state = last_hidden_state[mask_index]\n",
    "    top_values, top_indices = torch.topk(mask_hidden_state, k=5, dim=0)\n",
    "    top_prob = F.softmax(top_values, dim=0)\n",
    "    top_words = [tokenizer.decode(i.item()).strip() for i in top_indices]\n",
    "    list_of_list.append((top_words, top_indices.tolist(), top_prob.tolist()))\n",
    "\n",
    "list_of_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
